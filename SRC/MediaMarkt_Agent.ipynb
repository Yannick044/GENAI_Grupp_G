{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-27T14:12:38.325585Z",
     "start_time": "2025-10-27T14:12:38.316915Z"
    }
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Setup & Keys",
   "id": "9f095d847a3f4dc6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:12:38.543215Z",
     "start_time": "2025-10-27T14:12:38.528967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 1: Setup\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # l√§dt .env\n",
    "\n",
    "BASE_URL = \"https://api.cerebras.ai/v1\"\n",
    "LLM_MODEL = \"gpt-oss-120b\"\n",
    "LLM_TEMPERATURE = 0.3\n",
    "LLM_API_KEY = os.environ[\"CEREBRAS_API_KEY\"]\n"
   ],
   "id": "fe696e7230c5d0e7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LLM",
   "id": "370c03e0ca70b2dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:12:43.517327Z",
     "start_time": "2025-10-27T14:12:38.564487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=BASE_URL,\n",
    "    api_key=LLM_API_KEY,\n",
    "    model=LLM_MODEL,\n",
    "    temperature=LLM_TEMPERATURE,\n",
    ")"
   ],
   "id": "9cb3e49925466b67",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dokumente laden (gezielte URLs + User-Agent)",
   "id": "dce7eb174f6e4427"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:12:51.367962Z",
     "start_time": "2025-10-27T14:12:43.548230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 3: Dokumente laden (PDF, HTML, MD, TXT)\n",
    "from pathlib import Path\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.document_loaders import PyPDFLoader, BSHTMLLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# --- Einstellungen ---\n",
    "DATA_DIR = Path(\"../data\").resolve()\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# --- Text s√§ubern ---\n",
    "def clean_text(t: str) -> str:\n",
    "    t = t.replace(\"\\r\", \"\")\n",
    "    t = re.sub(r\"-\\n(\\w)\", r\"\\1\", t)      # Silbentrennungen entfernen\n",
    "    t = re.sub(r\"\\n{2,}\", \"\\n\\n\", t)      # doppelte Zeilenumbr√ºche\n",
    "    t = re.sub(r\"[ \\t]{2,}\", \" \", t)      # doppelte Leerzeichen\n",
    "    return t.strip()\n",
    "\n",
    "# --- Loader-Funktion ---\n",
    "def load_docs_from_raw():\n",
    "    assert RAW_DIR.exists(), f\"Ordner fehlt: {RAW_DIR.resolve()}\"\n",
    "    docs = []\n",
    "\n",
    "    for path in RAW_DIR.glob(\"*\"):\n",
    "        ext = path.suffix.lower()\n",
    "        if ext == \".pdf\":\n",
    "            try:\n",
    "                loader = PyPDFLoader(str(path))\n",
    "                pages = loader.load()\n",
    "            except Exception:\n",
    "                print(f\"‚ö†Ô∏è Fallback f√ºr PDF: {path.name}\")\n",
    "                continue\n",
    "            for d in pages:\n",
    "                d.metadata[\"source\"] = path.name\n",
    "                d.page_content = clean_text(d.page_content)\n",
    "                docs.append(d)\n",
    "\n",
    "        elif ext in (\".html\", \".htm\"):\n",
    "            try:\n",
    "                loader = BSHTMLLoader(str(path), open_encoding=\"utf-8\", bs_kwargs={\"features\": \"lxml\"})\n",
    "                page_docs = loader.load()\n",
    "            except Exception:\n",
    "                # Fallback ‚Äì manuell mit BeautifulSoup\n",
    "                raw = path.read_bytes()\n",
    "                soup = BeautifulSoup(raw, \"html.parser\")\n",
    "                text = soup.get_text(\"\\n\")\n",
    "                page_docs = [Document(page_content=text, metadata={\"source\": path.name})]\n",
    "            for d in page_docs:\n",
    "                d.page_content = clean_text(d.page_content)\n",
    "                docs.append(d)\n",
    "\n",
    "        elif ext in (\".md\", \".txt\"):\n",
    "            loader = TextLoader(str(path), encoding=\"utf-8\")\n",
    "            page_docs = loader.load()\n",
    "            for d in page_docs:\n",
    "                d.metadata[\"source\"] = path.name\n",
    "                d.page_content = clean_text(d.page_content)\n",
    "                docs.append(d)\n",
    "\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è √úberspringe unbekanntes Format: {path.name}\")\n",
    "\n",
    "    print(f\"‚úÖ {len(docs)} Dokumente geladen.\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "# --- Split + Embeddings ---\n",
    "def build_chunks(docs, chunk_size=800, overlap=120):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "def build_faiss(chunks):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "    return FAISS.from_documents(chunks, embedding=embeddings)\n",
    "\n",
    "# --- Lade alles ---\n",
    "docs = load_docs_from_raw()\n",
    "print(\"üîπ Beispieltext:\", docs[0].page_content[:300])\n",
    "splits = build_chunks(docs)\n",
    "vs = build_faiss(splits)\n",
    "print(\"FAISS Index erstellt mit:\", vs.index.ntotal, \"Vektoren\")\n"
   ],
   "id": "f07b690d4ca4db71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 24 Dokumente geladen.\n",
      "üîπ Beispieltext: Beratungstermin im Markt Die MediaMarkt App Lieferung zum Wunschtermin\n",
      "Alle Angebote und Aktionen (32 Artikel)\n",
      "Aus unserer Werbung\n",
      "154\n",
      "CHF¬†1069.‚Äì\n",
      "Bezahle in 36 Raten √† CHF¬†35.60\n",
      "Online verf√ºgbar\n",
      "Lieferung 28.10.2025 - 29.10.2025\n",
      "Abholung\n",
      "Bitte w√§hle einen Markt aus\n",
      "Produktdatenblatt\n",
      "APPLE iPhone 16 \n",
      "FAISS Index erstellt mit: 117 Vektoren\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:12:51.394913Z",
     "start_time": "2025-10-27T14:12:51.390207Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "cae9684151a31013",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Splitten",
   "id": "bcba9940c9cb4b3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:12:51.432403Z",
     "start_time": "2025-10-27T14:12:51.415689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 4: Split\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=120)\n",
    "splits = splitter.split_documents(docs)\n",
    "\n",
    "len(splits), len(docs), sum(len(s.page_content) for s in splits) // max(1, len(splits))\n"
   ],
   "id": "3ef533d2dbcec1b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, 24, 522)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Embeddings + FAISS",
   "id": "33a41c9c1e834899"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:13:05.907746Z",
     "start_time": "2025-10-27T14:12:51.457543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# FAISS direkt aus Dokumenten bauen\n",
    "vs = FAISS.from_documents(splits, embedding=embeddings)\n",
    "\n",
    "print(\"FAISS Vektoren:\", vs.index.ntotal)\n"
   ],
   "id": "c0307cc6df30e9ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS Vektoren: 157\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Prompt/Safeguard",
   "id": "b63a8d297ed51b1a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:13:05.954315Z",
     "start_time": "2025-10-27T14:13:05.921276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "SAFE_SYSTEM_PROMPT = \"\"\"\n",
    "Rolle & Aufgabe:\n",
    "Du bist eine freundliche und sachliche Kundenberaterin des MediaMarkt Onlineshops (Schweiz).\n",
    "Deine Aufgabe ist es, Kund:innen bei Fragen zu Produkten, Bestellungen, R√ºckgaben und allgemeinen Website-Themen zu helfen.\n",
    "\n",
    "Verhaltensregeln (sehr wichtig):\n",
    "1. Antworte ausschliesslich auf Basis der unten angegebenen Wissensquellen (Kontext).\n",
    "2. Wenn im Kontext keine passende Information steht, sag h√∂flich:\n",
    "   \"Dazu habe ich leider keine Informationen. Bitte wende dich direkt an den MediaMarkt Kundendienst.\"\n",
    "3. Erfinde oder erg√§nze keine eigenen Fakten. Keine Spekulationen.\n",
    "4. Bei rechtlichen oder Garantie-Themen zitiere nur Textstellen aus dem Kontext.\n",
    "5. Antworte kurz, klar und freundlich auf Deutsch (Schweiz), maximal 3‚Äì5 S√§tze.\n",
    "6. Verwende neutrale, respektvolle Sprache und duze die Kund:innen konsequent.\n",
    "7. Liste am Ende h√∂chstens drei relevante Quellen unter der √úberschrift \"Quellen:\" auf.\n",
    "\n",
    "Kontextinformationen:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SAFE_SYSTEM_PROMPT),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n"
   ],
   "id": "793fb2bed6a52c58",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Retriever + Antwort",
   "id": "30e42dc2b3d08883"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:13:05.976679Z",
     "start_time": "2025-10-27T14:13:05.967183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _retrieve(vs, query: str, k: int = 6):\n",
    "    retriever = vs.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": k, \"fetch_k\": 24, \"lambda_mult\": 0.5},\n",
    "    )\n",
    "    # LC 0.1.x hat get_relevant_documents; LC 0.2.x nutzt invoke()\n",
    "    try:\n",
    "        return retriever.get_relevant_documents(query)  # 0.1.x\n",
    "    except AttributeError:\n",
    "        return retriever.invoke(query)                  # 0.2.x+\n",
    "\n",
    "def answer_with_rag(question: str, k: int = 6) -> str:\n",
    "    docs = _retrieve(vs, question, k=k)\n",
    "    if not docs:\n",
    "        return \"Dazu habe ich keine gesicherten Infos in den FAQs.\"\n",
    "\n",
    "    # Kontext + Quellen bauen\n",
    "    context = \"\\n\\n\".join(d.page_content for d in docs[:k])\n",
    "    seen, sources = set(), []\n",
    "    for d in docs:\n",
    "        src = d.metadata.get(\"source\")\n",
    "        if src and src not in seen:\n",
    "            sources.append(src); seen.add(src)\n",
    "        if len(sources) == 3:\n",
    "            break\n",
    "\n",
    "    # Prompt und LLM-Aufruf (achte auf den korrekten Import deiner Version)\n",
    "    try:\n",
    "        from langchain_core.prompts import ChatPromptTemplate\n",
    "    except ModuleNotFoundError:\n",
    "        from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "    SAFE_SYSTEM_PROMPT = (\n",
    "        \"Du bist Kundenberater:in f√ºr MediaMarkt Onlineshop CH.\\n\"\n",
    "        \"Antworte NUR auf Basis des Kontexts. Wenn nichts passt, sag das offen.\\n\"\n",
    "        \"Erfinde nichts; gib keine rechtlichen Bewertungen. Zitiere bei Garantie/AGB knappe Textstellen.\\n\"\n",
    "        \"Kurz & pr√§zise; am Ende max. 3 Quellen unter 'Quellen:'.\\n\\n\"\n",
    "        \"Kontext:\\n{context}\"\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", SAFE_SYSTEM_PROMPT),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "    msgs = prompt.format_messages(context=context, question=question)\n",
    "    res = llm.invoke(msgs).content\n",
    "\n",
    "    if sources:\n",
    "        res += \"\\n\\nQuellen:\\n\" + \"\\n\".join(f\"- {s}\" for s in sources)\n",
    "    return res\n"
   ],
   "id": "f5aca2bdcc7a049a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Testfragen",
   "id": "d50c600f85383c86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:13:56.132668Z",
     "start_time": "2025-10-27T14:13:05.987613Z"
    }
   },
   "cell_type": "code",
   "source": "print(answer_with_rag(\"Welche Online-Zahlungsmethoden gibt es?\"))",
   "id": "43edd84342794424",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im MediaMarkt‚ÄëOnlineshop kannst du online unter anderem mit folgenden Zahlungsmethoden bezahlen:\n",
      "\n",
      "- **Kreditkarte**  \n",
      "- **PayPal**  \n",
      "- **Twint**  \n",
      "- **Google‚ÄØPay**  \n",
      "- **Kauf auf Rechnung** (nachtr√§glich per Rechnung zahlen)\n",
      "\n",
      "Quelle:‚ÄØMediaMarkt‚ÄØCH ‚Äì‚ÄØ‚ÄûZahlungsarten‚Äú‚ÄØ(https://www.mediamarkt.ch/de/service/zahlungsarten)  \n",
      "\n",
      "Quellen:\n",
      "1. https://www.mediamarkt.ch/de/service/zahlungsarten\n",
      "\n",
      "Quellen:\n",
      "- zahlungsarten _ mediamarkt.pdf\n",
      "- C:\\Users\\simon\\OneDrive\\FH\\BAI\\GENAI_Grupp_G\\data\\raw\\Zahlungsarten _ MediaMarkt.html\n",
      "- C:\\Users\\simon\\OneDrive\\FH\\BAI\\GENAI_Grupp_G\\data\\raw\\garantie_und_versicherung _ MediaMarkt.html\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:13:57.467831Z",
     "start_time": "2025-10-27T14:13:56.194436Z"
    }
   },
   "cell_type": "code",
   "source": "print(answer_with_rag(\"Wie lange ist die Retourenfrist?\"))\n",
   "id": "b6c6add21328dd80",
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'message': \"We're experiencing high traffic right now! Please try again soon.\", 'type': 'too_many_requests_error', 'param': 'queue', 'code': 'queue_exceeded'}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRateLimitError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[43manswer_with_rag\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mWie lange ist die Retourenfrist?\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 47\u001B[39m, in \u001B[36manswer_with_rag\u001B[39m\u001B[34m(question, k)\u001B[39m\n\u001B[32m     42\u001B[39m prompt = ChatPromptTemplate.from_messages([\n\u001B[32m     43\u001B[39m     (\u001B[33m\"\u001B[39m\u001B[33msystem\u001B[39m\u001B[33m\"\u001B[39m, SAFE_SYSTEM_PROMPT),\n\u001B[32m     44\u001B[39m     (\u001B[33m\"\u001B[39m\u001B[33mhuman\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[38;5;132;01m{question}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     45\u001B[39m ])\n\u001B[32m     46\u001B[39m msgs = prompt.format_messages(context=context, question=question)\n\u001B[32m---> \u001B[39m\u001B[32m47\u001B[39m res = \u001B[43mllm\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsgs\u001B[49m\u001B[43m)\u001B[49m.content\n\u001B[32m     49\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m sources:\n\u001B[32m     50\u001B[39m     res += \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mQuellen:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m + \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.join(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m- \u001B[39m\u001B[38;5;132;01m{\u001B[39;00ms\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m sources)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniforge3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:379\u001B[39m, in \u001B[36mBaseChatModel.invoke\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    365\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    366\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minvoke\u001B[39m(\n\u001B[32m    367\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    372\u001B[39m     **kwargs: Any,\n\u001B[32m    373\u001B[39m ) -> AIMessage:\n\u001B[32m    374\u001B[39m     config = ensure_config(config)\n\u001B[32m    375\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[32m    376\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAIMessage\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    377\u001B[39m         cast(\n\u001B[32m    378\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mChatGeneration\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m--> \u001B[39m\u001B[32m379\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    380\u001B[39m \u001B[43m                \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    381\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    382\u001B[39m \u001B[43m                \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcallbacks\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    383\u001B[39m \u001B[43m                \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtags\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    384\u001B[39m \u001B[43m                \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetadata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    385\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_name\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    386\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_id\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    387\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    388\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m.generations[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m],\n\u001B[32m    389\u001B[39m         ).message,\n\u001B[32m    390\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniforge3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1088\u001B[39m, in \u001B[36mBaseChatModel.generate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m   1079\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m   1080\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_prompt\u001B[39m(\n\u001B[32m   1081\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1085\u001B[39m     **kwargs: Any,\n\u001B[32m   1086\u001B[39m ) -> LLMResult:\n\u001B[32m   1087\u001B[39m     prompt_messages = [p.to_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m-> \u001B[39m\u001B[32m1088\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniforge3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:903\u001B[39m, in \u001B[36mBaseChatModel.generate\u001B[39m\u001B[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m    900\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages):\n\u001B[32m    901\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    902\u001B[39m         results.append(\n\u001B[32m--> \u001B[39m\u001B[32m903\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    904\u001B[39m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    905\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    906\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    907\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    908\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    909\u001B[39m         )\n\u001B[32m    910\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    911\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniforge3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1192\u001B[39m, in \u001B[36mBaseChatModel._generate_with_cache\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1190\u001B[39m     result = generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[32m   1191\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m inspect.signature(\u001B[38;5;28mself\u001B[39m._generate).parameters.get(\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1192\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1193\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m   1194\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1195\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1196\u001B[39m     result = \u001B[38;5;28mself\u001B[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniforge3\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1299\u001B[39m, in \u001B[36mBaseChatOpenAI._generate\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1297\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m raw_response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(raw_response, \u001B[33m\"\u001B[39m\u001B[33mhttp_response\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m   1298\u001B[39m         e.response = raw_response.http_response  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1299\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m   1300\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   1301\u001B[39m     \u001B[38;5;28mself\u001B[39m.include_response_headers\n\u001B[32m   1302\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m raw_response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1303\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(raw_response, \u001B[33m\"\u001B[39m\u001B[33mheaders\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   1304\u001B[39m ):\n\u001B[32m   1305\u001B[39m     generation_info = {\u001B[33m\"\u001B[39m\u001B[33mheaders\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mdict\u001B[39m(raw_response.headers)}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniforge3\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1294\u001B[39m, in \u001B[36mBaseChatOpenAI._generate\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1287\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m _construct_lc_result_from_responses_api(\n\u001B[32m   1288\u001B[39m             response,\n\u001B[32m   1289\u001B[39m             schema=original_schema_obj,\n\u001B[32m   1290\u001B[39m             metadata=generation_info,\n\u001B[32m   1291\u001B[39m             output_version=\u001B[38;5;28mself\u001B[39m.output_version,\n\u001B[32m   1292\u001B[39m         )\n\u001B[32m   1293\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1294\u001B[39m         raw_response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwith_raw_response\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mpayload\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1295\u001B[39m         response = raw_response.parse()\n\u001B[32m   1296\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniforge3\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001B[39m, in \u001B[36mto_raw_response_wrapper.<locals>.wrapped\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    360\u001B[39m extra_headers[RAW_RESPONSE_HEADER] = \u001B[33m\"\u001B[39m\u001B[33mtrue\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    362\u001B[39m kwargs[\u001B[33m\"\u001B[39m\u001B[33mextra_headers\u001B[39m\u001B[33m\"\u001B[39m] = extra_headers\n\u001B[32m--> \u001B[39m\u001B[32m364\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m cast(LegacyAPIResponse[R], \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniforge3\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001B[39m, in \u001B[36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    284\u001B[39m             msg = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[32m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    285\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[32m--> \u001B[39m\u001B[32m286\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniforge3\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1156\u001B[39m, in \u001B[36mCompletions.create\u001B[39m\u001B[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m   1110\u001B[39m \u001B[38;5;129m@required_args\u001B[39m([\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m], [\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m   1111\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcreate\u001B[39m(\n\u001B[32m   1112\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1153\u001B[39m     timeout: \u001B[38;5;28mfloat\u001B[39m | httpx.Timeout | \u001B[38;5;28;01mNone\u001B[39;00m | NotGiven = not_given,\n\u001B[32m   1154\u001B[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001B[32m   1155\u001B[39m     validate_response_format(response_format)\n\u001B[32m-> \u001B[39m\u001B[32m1156\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1157\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/chat/completions\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1158\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1159\u001B[39m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m   1160\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmessages\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1161\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodel\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1162\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43maudio\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43maudio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1163\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfrequency_penalty\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1164\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfunction_call\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1165\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfunctions\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1166\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlogit_bias\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1167\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlogprobs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1168\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmax_completion_tokens\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_completion_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1169\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmax_tokens\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1170\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetadata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1171\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodalities\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodalities\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1172\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mn\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1173\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mparallel_tool_calls\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1174\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mprediction\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprediction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1175\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpresence_penalty\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1176\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mprompt_cache_key\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt_cache_key\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1177\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mreasoning_effort\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mreasoning_effort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1178\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mresponse_format\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1179\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msafety_identifier\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43msafety_identifier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1180\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mseed\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1181\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mservice_tier\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mservice_tier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1182\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstop\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1183\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstore\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1184\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstream\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1185\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstream_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1186\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtemperature\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1187\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtool_choice\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1188\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtools\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1189\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtop_logprobs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1190\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtop_p\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1191\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43muser\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1192\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mverbosity\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbosity\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1193\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mweb_search_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mweb_search_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1194\u001B[39m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1195\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCompletionCreateParamsStreaming\u001B[49m\n\u001B[32m   1196\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\n\u001B[32m   1197\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCompletionCreateParamsNonStreaming\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1198\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1199\u001B[39m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1200\u001B[39m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\n\u001B[32m   1201\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1202\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1203\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1204\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1205\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniforge3\\Lib\\site-packages\\openai\\_base_client.py:1259\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[39m\n\u001B[32m   1245\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1246\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1247\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1254\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1255\u001B[39m ) -> ResponseT | _StreamT:\n\u001B[32m   1256\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1257\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001B[32m   1258\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1259\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniforge3\\Lib\\site-packages\\openai\\_base_client.py:1047\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1044\u001B[39m             err.response.read()\n\u001B[32m   1046\u001B[39m         log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1047\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1049\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m   1051\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mcould not resolve response (should never happen)\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[31mRateLimitError\u001B[39m: Error code: 429 - {'message': \"We're experiencing high traffic right now! Please try again soon.\", 'type': 'too_many_requests_error', 'param': 'queue', 'code': 'queue_exceeded'}"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:13:57.475657700Z",
     "start_time": "2025-10-27T11:01:18.283463Z"
    }
   },
   "cell_type": "code",
   "source": "print(answer_with_rag(\"Ist das Samsung Galaxy S25 aktuell im Angebot und habe ich Garantie darauf?\"))",
   "id": "233807f65fc6bb38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ja, das **Samsung‚ÄØGalaxy‚ÄØS25‚ÄØEdge‚ÄØ512‚ÄØGB** ist aktuell im Angebot‚ÄØ‚Äì‚ÄØes wird mit **‚Äë50‚ÄØ%** (CHF‚ÄØ629‚ÄØ.‚Äì) und einer Ratenzahlung von 23‚ÄØ√ó‚ÄØCHF‚ÄØ30.70 beworben.  \n",
      "\n",
      "**Garantie:**  \n",
      "Alle bei MediaMarkt gekauften Ger√§te unterliegen der gesetzlichen Gew√§hrleistung (2‚ÄØJahre) und k√∂nnen zus√§tzlich √ºber die Rubrik **‚ÄûGarantie‚ÄØ&‚ÄØVersicherungen‚Äú** versichert werden.  \n",
      "\n",
      "**Quellen**  \n",
      "- Angebot‚ÄëAnzeige: ‚ÄûSAMSUNG Galaxy S25 Edge 512‚ÄØGB ‚Ä¶ CHF‚ÄØ629‚ÄØ.‚Äì‚ÄØBezahle in 23‚ÄØRaten ‚Ä¶‚Äú  \n",
      "- Service‚ÄëBereich: ‚ÄûGarantie‚ÄØ&‚ÄØVersicherungen‚Äú (MediaMarkt‚ÄëOnlineshop)  \n",
      "\n",
      "Quellen:\n",
      "- Angebote.pdf\n",
      "- C:\\Users\\simon\\OneDrive\\FH\\BAI\\GENAI_Grupp_G\\data\\raw\\garantie_und_versicherung _ MediaMarkt.html\n",
      "- C:\\Users\\simon\\OneDrive\\FH\\BAI\\GENAI_Grupp_G\\data\\raw\\Zahlungsarten _ MediaMarkt.html\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:13:57.477632Z",
     "start_time": "2025-10-27T13:57:18.302748Z"
    }
   },
   "cell_type": "code",
   "source": "print(answer_with_rag(\"Kannst du mir das Iphone 17 Ultra empfehlen?\"))",
   "id": "8fe2dc3ff8f93e16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leider habe ich in unserem aktuellen MediaMarkt‚ÄëOnline‚ÄëKatalog keinen Eintrag f√ºr ein iPhone‚ÄØ17‚ÄØUltra gefunden. Wir f√ºhren derzeit iPhone‚ÄØ17‚ÄëModelle (z.‚ÄØB. iPhone‚ÄØ17, iPhone‚ÄØ17‚ÄØPro, iPhone‚ÄØ17‚ÄØPro‚ÄØMax), aber kein Ultra‚ÄëModell.\n",
      "\n",
      "Falls Sie Fragen zu den verf√ºgbaren iPhone‚ÄëModellen haben oder ein anderes Ger√§t vergleichen m√∂chten, k√∂nnen Sie gern unser Sortiment durchsuchen.\n",
      "\n",
      "**Quellen:**\n",
      "- MediaMarkt‚ÄëPromotions‚Äë√úbersicht (Liste aller Angebote)‚ÄØ[https://www.mediamarkt.ch/de/list/mm_promotions_all?query=mm_promotions_all&category=CAT_CH_MM_680760&page=3]\n",
      "\n",
      "Quellen:\n",
      "- Angebote.pdf\n",
      "- C:\\Users\\simon\\OneDrive\\FH\\BAI\\GENAI_Grupp_G\\data\\raw\\garantie_und_versicherung _ MediaMarkt.html\n",
      "- C:\\Users\\simon\\OneDrive\\FH\\BAI\\GENAI_Grupp_G\\data\\raw\\Zahlungsarten _ MediaMarkt.html\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:15:49.018643Z",
     "start_time": "2025-10-27T14:15:46.304868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gradio as gr\n",
    "\n",
    "# --- Gradio UI ---\n",
    "def chat_interface(user_question):\n",
    "    # Ruft euren bestehenden RAG-Agenten auf:\n",
    "    response = answer_with_rag(user_question)\n",
    "    return response\n",
    "\n",
    "# --- Custom CSS f√ºr MediaMarkt Style ---\n",
    "custom_css = \"\"\"\n",
    "body {\n",
    "    background-color: #ffffff;\n",
    "    font-family: 'Helvetica', sans-serif;\n",
    "}\n",
    ".gradio-container {\n",
    "    border: 3px solid #e60000;\n",
    "    border-radius: 20px;\n",
    "    padding: 25px;\n",
    "    background: linear-gradient(180deg, #ffffff 0%, #ffecec 100%);\n",
    "    box-shadow: 0 0 25px rgba(230, 0, 0, 0.1);\n",
    "}\n",
    "h1, h2, h3 {\n",
    "    color: #e60000;\n",
    "    text-align: center;\n",
    "    font-weight: 900;\n",
    "    text-transform: uppercase;\n",
    "}\n",
    "textarea {\n",
    "    border: 2px solid #e60000 !important;\n",
    "    border-radius: 12px !important;\n",
    "    padding: 12px !important;\n",
    "    background-color: #fffafa !important;\n",
    "    font-size: 16px !important;\n",
    "}\n",
    "button {\n",
    "    background-color: #e60000 !important;\n",
    "    color: #ffffff !important;\n",
    "    border-radius: 12px !important;\n",
    "    border: none !important;\n",
    "    font-weight: bold !important;\n",
    "    transition: all 0.3s ease-in-out !important;\n",
    "}\n",
    "button:hover {\n",
    "    background-color: #b30000 !important;\n",
    "    transform: scale(1.05);\n",
    "}\n",
    ".output-markdown {\n",
    "    background-color: #fffafa !important;\n",
    "    border: 1px solid #e60000 !important;\n",
    "    border-radius: 12px !important;\n",
    "    padding: 15px !important;\n",
    "}\n",
    "footer {\n",
    "    text-align: center;\n",
    "    font-size: 13px;\n",
    "    color: #666;\n",
    "    margin-top: 15px;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=chat_interface,\n",
    "    inputs=gr.Textbox(\n",
    "        label=\"üí¨ Frage:\",\n",
    "        placeholder=\"Frag den MediaMarkt Kundenservice...\",\n",
    "        lines=2,\n",
    "    ),\n",
    "    outputs=gr.Markdown(label=\"üìñ Antwort\"),\n",
    "    title=\"üõí MediaMarkt KI-Kundenservice\",\n",
    "    description=\"Dieser KI-Agent beantwortet Fragen auf Basis echter MediaMarkt-Dokumente (RAG + Safeguarding).\",\n",
    "    theme=\"gradio/soft\",\n",
    "    examples=[\n",
    "        [\"Wie lange ist die Retourenfrist?\"],\n",
    "        [\"Wie reklamiere ich ein defektes Produkt?\"],\n",
    "        [\"Welche Zahlungsm√∂glichkeiten gibt es?\"],\n",
    "    ],\n",
    "    css=custom_css,  # <- MediaMarkt Design anwenden\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=False)\n"
   ],
   "id": "54952c7ef18b0da3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tool Testing (Function Calling) - nicht ausgereift",
   "id": "c872f84a7e0768b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 10,
   "source": [
    "from langchain.tools import tool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ProductSearchInput(BaseModel):\n",
    "    keyword: str = Field(..., description=\"Produkt oder Marke, nach der gesucht werden soll (z. B. 'iPhone', 'Smartwatch', 'Samsung')\")\n",
    "\n",
    "@tool(args_schema=ProductSearchInput)\n",
    "def get_best_offer(keyword: str) -> str:\n",
    "    \"\"\"Findet das beste Angebot zu einem bestimmten Produkt oder einer Marke basierend auf gespeicherten Textdaten.\"\"\"\n",
    "    keyword_lower = keyword.lower()\n",
    "    matches = [doc for doc in docs if keyword_lower in doc.page_content.lower()]\n",
    "\n",
    "    if not matches:\n",
    "        return {\"result\": f\"Kein Angebot f√ºr '{keyword}' gefunden.\"}\n",
    "\n",
    "    # Heuristik: w√§hle das l√§ngste oder relevanteste Textst√ºck\n",
    "    best_doc = max(matches, key=lambda d: len(d.page_content))\n",
    "\n",
    "    # Extrahiere eine Preiszeile (rudiment√§r)\n",
    "    import re\n",
    "    price_match = re.search(r\"CHF\\s*[\\d'‚Äô.,]+\", best_doc.page_content)\n",
    "    price_info = price_match.group(0) if price_match else \"Preis nicht erkannt\"\n",
    "\n",
    "    snippet = best_doc.page_content[:400].strip().replace(\"\\n\", \" \")\n",
    "    return {\"result\": f\"üîç Gefundenes Angebot f√ºr '{keyword}': {price_info}\\n\\n{snippet}...\"}"
   ],
   "id": "ab242adbba581ea3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': \"üîç Gefundenes Angebot f√ºr 'Galaxy': CHF 158.\\n\\nGarantie und Versicherung | MediaMarkt  Zum Hauptinhalt wechselnAlle KategorienWas suchst du?Mein MarktKein Markt ausgew√§hltlanguageSwitch.voiceOverDescription: DeutschdeMen√ºAngebotemyMediaMarktServiceGeschenkkarteMediaMagazinEigenmarkenMarkenshops% Outlet %Shopping CardSmartbarJobsHilfeAlle KategorienAngebote & Aktionen Angebote & AktionenZu Angebote & AktionenAktionenOutletEintausch RabattForeve...\"}\n"
     ]
    }
   ],
   "execution_count": 11,
   "source": "print(get_best_offer.invoke({\"keyword\": \"Galaxy\"}))",
   "id": "96bbb649e5941374"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
